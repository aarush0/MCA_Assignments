# -*- coding: utf-8 -*-
"""mca_word2vec.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1nJAdgDQiBr8Sjk4Daoic55m09fBeap4P
"""

'''
Reference to original code:

https://github.com/adventuresinML/adventures-in-ml-code/blob/e661eeb5db86d2d0aa21621b68b5186d80e3d8b6/keras_word2vec.py#L37

'''

import nltk
nltk.download('abc')
nltk.download('punkt')

from nltk.corpus import abc
import re
import numpy as np
from keras.preprocessing.sequence import skipgrams
from keras.preprocessing import sequence

from keras.models import Model
from keras.layers import Input, Dense, Reshape, Dot, Lambda
from keras.layers.embeddings import Embedding
from keras.preprocessing.sequence import skipgrams
from keras.preprocessing import sequence
import keras.backend as K

import urllib
import collections
import os
import zipfile

import numpy as np
import tensorflow as tf

vocabulary = nltk.corpus.abc.words()
vocabulary_size = 30000

count = [['X', -1]]
count.extend(collections.Counter(vocabulary).most_common(vocabulary_size))

#print(count)

word_to_id = dict()
for word, temp in count:
    word_to_id[word] = len(word_to_id)

#print(word_to_id)

data = list()

for word in vocabulary:
    if word in word_to_id:
        index = word_to_id[word]
    else:
        index = 0
    data.append(index)

id_to_word = dict(zip(word_to_id.values(), word_to_id.keys()))

#print(id_to_word)

window_size = 3
vector_dim = 200
epochs = 20000

valid_size = 12    
valid_window = 100  

sampling_table = sequence.make_sampling_table(vocabulary_size)
couples, labels = skipgrams(data, vocabulary_size, window_size=window_size, sampling_table=sampling_table)
word_target, word_context = zip(*couples)
word_target = np.array(word_target, dtype="int32")
word_context = np.array(word_context, dtype="int32")

#print(couples, labels)

input_target = Input((1,))
input_context = Input((1,))

embedding = Embedding(vocabulary_size, vector_dim, input_length=1, name='embedding')
target = embedding(input_target)
target = Reshape((vector_dim, 1))(target)
context = embedding(input_context)
context = Reshape((vector_dim, 1))(context)

similarity = Dot(axes=1, normalize = True)([target, context])
dot_product = Dot(axes=1)([target, context])

dot_product = Reshape((1,))(dot_product)
output = Dense(1, activation='sigmoid')(dot_product)
model = Model(input=[input_target, input_context], output=output)
model.compile(loss='binary_crossentropy', optimizer='adadelta')

#model.summary()

similarity_model = Model(input=[input_target, input_context], output=similarity)

Target = np.zeros((100,))
Context = np.zeros((100,))
Label = np.zeros((100,))

for cnt in range(epochs):
    for tm in range(100):
      idx = np.random.randint(0, len(labels)-1)
      Target[tm,] = word_target[idx]
      Context[tm,] = word_context[idx]
      Label[tm,] = labels[idx]

    loss = model.train_on_batch([Target, Context], Label)
    
    if cnt % 100 == 0:
        print(cnt, loss)

check_words = ['Iraq', 'wheat', 'letters', 'government', 'Australia', 'Federal', 'Western', 'million', 'the', 'is']

for i in range(len(check_words)):
    valid_word = check_words[i]
    k = 8 
    sim = np.zeros((vocabulary_size,))
    in_target = np.zeros((1,))
    in_context = np.zeros((1,))
    in_target[0,] = dictionary[check_words[i]]

    for j in range(vocabulary_size):
        in_context[0,] = j
        out = similarity_model.predict_on_batch([in_target, in_context])
        sim[j] = out

    nearest = (-sim).argsort()[1:k + 1]
    
    print("Similar to "+valid_word)
    for K in range(k):
        close_word = id_to_word[nearest[K]]
        print(close_word, end = " ")
    print()